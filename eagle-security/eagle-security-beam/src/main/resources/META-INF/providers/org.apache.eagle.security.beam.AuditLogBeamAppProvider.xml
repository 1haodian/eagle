<?xml version="1.0" encoding="UTF-8" ?>
<!--
  ~ Licensed to the Apache Software Foundation (ASF) under one or more
  ~ contributor license agreements.  See the NOTICE file distributed with
  ~ this work for additional information regarding copyright ownership.
  ~ The ASF licenses this file to You under the Apache License, Version 2.0
  ~ (the "License"); you may not use this file except in compliance with
  ~ the License.  You may obtain a copy of the License at
  ~
  ~    http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<application>
    <type>AUDIT_LOG_MONITOR_BEAM</type>
    <name>Audit Log Monitor Beam</name>
    <appClass>org.apache.eagle.security.beam.AuditLogBeamAppProvider</appClass>
    <configuration>
        <property>
            <name>appId</name>
            <displayName>Eagle App Id on beam</displayName>
            <value>auditlog_beam</value>
            <description>eagle app id on beam</description>
            <required>true</required>
        </property>
        <!-- data source configurations -->
        <property>
            <name>dataSourceConfig.topicList</name>
            <displayName>Kafka Topic for Data Source</displayName>
            <value>audit_log1,audit_log2,audit_log3</value>
            <description>data source topic list, separated by comma</description>
            <required>true</required>
        </property>
        <property>
            <name>dataSourceConfig.ZkConnection</name>
            <displayName>Kafka Zookeeper Connection</displayName>
            <value>sandbox.hortonworks.com:2181</value>
            <description>kafka zk connection</description>
            <required>true</required>
        </property>
        <!-- data sink configurations -->
        <property>
            <name>dataSinkConfig.topic</name>
            <displayName>Kafka Topic for Parsed Data Sink</displayName>
            <value>hdfs_audit_log_enriched</value>
            <description>topic for kafka data sink</description>
            <required>true</required>
        </property>
        <property>
            <name>dataSinkConfig.brokerList</name>
            <displayName>Kafka BrokerList for Data Sink</displayName>
            <value>sandbox.hortonworks.com:6667</value>
            <description>kafka broker list</description>
            <required>true</required>
        </property>
        <property>
            <name>autoOffsetResetConfig</name>
            <displayName>autoOffsetResetConfig</displayName>
            <value>largest</value>
            <description>autoOffsetResetConfig</description>
            <required>true</required>
        </property>
        <property>
            <name>sparkRunner.checkpoint</name>
            <displayName>Beam SparkRunner Checkpoint Path</displayName>
            <value>/user/checkpoint</value>
            <description>sparkrunner checkpoint</description>
        </property>
        <property>
            <name>sparkRunner.master</name>
            <displayName>Beam SparkRunner Master</displayName>
            <value>local[4]</value>
            <description>sparkrunner master</description>
        </property>
        <property>
            <name>topology.core</name>
            <displayName>Spark executor core</displayName>
            <value>1</value>
            <description>spark.executor.cores</description>
        </property>
        <property>
            <name>topology.memory</name>
            <displayName>Spark executor memory</displayName>
            <value>5g</value>
            <description>spark.executor.memory(1g,2g,8g)</description>
        </property>
        <property>
            <name>topology.numOfRouterBolts</name>
            <displayName>RouterBolt num</displayName>
            <value>4</value>
            <description>RouterBolt num</description>
        </property>
        <property>
            <name>topology.numOfAlertBolts</name>
            <displayName>AlertBolt num</displayName>
            <value>10</value>
            <description>AlertBolt num</description>
            <required>true</required>
        </property>
        <property>
            <name>topology.numOfPublishTasks</name>
            <displayName>PublishTasks num</displayName>
            <value>4</value>
            <description>PublishTasks num</description>
        </property>
        <property>
            <name>topology.master</name>
            <displayName>Spark streaming master</displayName>
            <value>yarn</value>
            <description>Spark streaming master(local[4],local[*],yarn,spark://HOST:PORT)</description>
        </property>
        <property>
            <name>topology.driverMemory</name>
            <displayName>Spark driver memory</displayName>
            <value>5g</value>
            <description>spark.driver.memory(1b,1k,1m,1g,1t,1p)</description>
        </property>
        <property>
            <name>topology.driverCores</name>
            <displayName>Spark driver cores number</displayName>
            <value>4</value>
            <description>spark.driver.cores</description>
        </property>
        <property>
            <name>topology.deployMode</name>
            <displayName>Spark steaming deployMode</displayName>
            <value>client</value>
            <description>spark.submit.deployMode(client,cluster)</description>
        </property>
        <property>
            <name>topology.groupId</name>
            <displayName>Kafka Consumer ID</displayName>
            <value>eagle_consumer</value>
            <description>Kafka Consumer ID(group.id)</description>
        </property>
        <property>
            <name>topology.name</name>
            <displayName>Spark steaming app name</displayName>
            <value>Eagle Alert Engine On SparkStreaming</value>
            <description>Spark steaming app name</description>
        </property>
        <property>
            <name>topology.dynamicAllocation</name>
            <displayName>Spark streaming dynamicAllocation config</displayName>
            <value>true</value>
            <description>spark.streaming.dynamicAllocation.enable</description>
        </property>
        <property>
            <name>topology.offsetreset</name>
            <displayName>Kafka auto offset reset config</displayName>
            <value>largest</value>
            <description>Auto offset reset config for Kafka(largest,smallest)</description>
        </property>
        <property>
            <name>metadataService.context</name>
            <displayName>MetadataService context</displayName>
            <value>/rest</value>
            <description>MetadataService context</description>
        </property>
        <property>
            <name>metadataService.port</name>
            <displayName>MetadataService port</displayName>
            <value>9090</value>
            <description>MetadataService port</description>
        </property>
        <property>
            <name>metadataService.host</name>
            <displayName>MetadataService host</displayName>
            <value>localhost</value>
            <description>MetadataService host</description>
        </property>
        <property>
            <name>topology.mainclass</name>
            <displayName>Mainclass</displayName>
            <value>org.apache.eagle.security.beam.AuditLogBeamApplicationMain</value>
            <description>Mainclass</description>
        </property>
        <property>
            <name>topology.sparkhome</name>
            <displayName>Spark home</displayName>
            <value>/opt/cloudera/parcels/YSPARK/lib/spark</value>
            <description>Spark home</description>
        </property>
        <property>
            <name>topology.sparkuiport</name>
            <displayName>Spark UI port</displayName>
            <value>4123</value>
            <description>Spark UI port</description>
        </property>
        <property>
            <name>topology.sparkconffilepath</name>
            <displayName>Spark conf file path</displayName>
            <value>/etc/spark/conf/spark-defaults.conf</value>
            <description>Spark conf file path</description>
        </property>
        <property>
            <name>topology.appresource</name>
            <displayName>Spark Appresource</displayName>
            <value>/home/tandem/eagledlg/eagle/lib/eagle-topology-0.5.0-SNAPSHOT-assembly.jar</value>
            <description>Spark Appresource</description>
        </property>
        <property>
            <name>topology.appjars</name>
            <displayName>Spark Appjars</displayName>
            <value>a.jar,b.jar</value>
            <description>Spark App Jars</description>
        </property>
        <property>
            <name>topology.yarnqueue</name>
            <displayName>spark.yarn.queue</displayName>
            <value>default</value>
            <description>spark.yarn.queue</description>
        </property>
        <property>
            <name>topology.verbose</name>
            <displayName>Spark verbose</displayName>
            <value>false</value>
            <description>Spark verbose</description>
        </property>
    </configuration>
    <streams>
        <stream>
            <streamId>audit_log_stream</streamId>
            <description>Audit Log Stream</description>
            <validate>true</validate>
            <timeseries>true</timeseries>
            <columns>
                <column>
                    <name>user</name>
                    <type>string</type>
                </column>
                <column>
                    <name>ip</name>
                    <type>string</type>
                </column>
                <column>
                    <name>jobId</name>
                    <type>string</type>
                </column>
                <column>
                    <name>operation</name>
                    <type>string</type>
                </column>
                <column>
                    <name>timestamp</name>
                    <type>long</type>
                </column>
            </columns>
        </stream>
    </streams>
    <docs>
        <install>
            # Step 1: Create source kafka topic named "${site}_example_source_topic"

            ./bin/kafka-topics.sh --create --topic example_source_topic --replication-factor 1 --replication 1

            # Step 2: Set up data collector to flow data into kafka topic in

            ./bin/logstash -f log_collector.conf

            ## `log_collector.conf` sample as following:

            input {

            }
            filter {

            }
            output{

            }

            # Step 3: start application

            # Step 4: monitor with featured portal or alert with policies
        </install>
        <uninstall>
            # Step 1: stop and uninstall application
            # Step 2: delete kafka topic named "${site}_example_source_topic"
            # Step 3: stop logstash
        </uninstall>
    </docs>
</application>
